{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270244e1-6875-4b46-9476-2d87eabf9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Island:\n",
    "    def __init__(self, xcentr=5, ycentr=5, radius=1):\n",
    "        self.xcentr = xcentr\n",
    "        self.ycentr = ycentr\n",
    "        self.radius = radius\n",
    "\n",
    "    def belongs_to_boarder(self, x, y):\n",
    "        dist = ((x - self.xcentr) ** 2 + (y - self.ycentr) ** 2) ** 0.5\n",
    "        if dist <= self.radius:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_dist(self, x, y):\n",
    "        return ((x - self.xcentr) ** 2 + (y - self.ycentr) ** 2) ** 0.5 - self.radius\n",
    "\n",
    "    def draw_island(self, color='blue'):\n",
    "        return plt.Circle((self.xcentr, self.ycentr), self.radius, color=color)\n",
    "\n",
    "    def getCoords(self):\n",
    "        return [self.xcentr, self.ycentr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c89a62-b8ce-432f-8f7c-e54f83d0f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target:\n",
    "    def __init__(self, xcentr=5, ycentr=5, radius=1, angle=math.pi/2, vel=10):\n",
    "        self.x = xcentr\n",
    "        self.y = ycentr\n",
    "        self.radius = radius\n",
    "        self.direction = angle\n",
    "        self.v = vel\n",
    "\n",
    "    def belongs_to_boarder(self, x, y):\n",
    "        dist = ((x - self.x) ** 2 + (y - self.y) ** 2) ** 0.5\n",
    "        if dist <= self.radius:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_dist(self, x, y):\n",
    "        return ((x - self.x) ** 2 + (y - self.y) ** 2) ** 0.5 - self.radius\n",
    "\n",
    "    def draw(self, color='blue'):\n",
    "        return plt.Circle((self.x, self.y), self.radius, color=color)\n",
    "\n",
    "    def getCoords(self):\n",
    "        return [self.x, self.y]\n",
    "    \n",
    "    def move(self, dt=1):\n",
    "        self.x += dt * self.v * math.cos(self.direction)\n",
    "        self.y += dt * self.v * math.sin(self.direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31785e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lower_bound = 0\n",
    "x_upper_bound = 10\n",
    "y_lower_bound = 0\n",
    "y_upper_bound = 10\n",
    "\n",
    "a = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cc8f2-273e-47b3-b737-a416e9072088",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a040072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ship:\n",
    "    def __init__(self, x = 1, y = 5, v = 1, direction = 0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.v = v\n",
    "        self.direction = direction\n",
    "        self.prev_d = self.direction\n",
    "        self.positions = [[self.x, self.y]]\n",
    "        self.cum_d = 0\n",
    "        \n",
    "    def move(self, dt=1):\n",
    "        self.x += dt * self.v * math.cos(self.direction)\n",
    "        self.y += dt * self.v * math.sin(self.direction)\n",
    "        self.cum_d += abs(self.direction - self.prev_d)\n",
    "        self.prev_d = self.direction\n",
    "        \n",
    "    def getCoords(self):\n",
    "        return [self.x, self.y]\n",
    "    \n",
    "    def add_position(self):\n",
    "        self.positions.append([self.x, self.y])\n",
    "        \n",
    "    def get_positions(self):\n",
    "        return self.positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521dd18-a193-48d9-9376-dd7e8ff44063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__()\n",
    "        self.action_space = ['left', 'right', 'idle']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.n_features = 2\n",
    "        \n",
    "        self.ship = None\n",
    "        self.target = None\n",
    "        self.flag = None\n",
    "\n",
    "        self.build_environment()\n",
    "        \n",
    "        self.route = []\n",
    "        self.list_of_route = []\n",
    "        self.d = {}\n",
    "        self.f = {}\n",
    "        \n",
    "        self.prev_dist = 0\n",
    "        self.prev_i_dist = 0\n",
    "        self.n_angles = 0\n",
    "\n",
    "        self.i = 0\n",
    "\n",
    "        self.c = True\n",
    "\n",
    "        self.longest = 0\n",
    "\n",
    "        self.shortest = 0\n",
    "        \n",
    "    def get_state(self):\n",
    "        a = self.ship.getCoords()\n",
    "        b = self.target.getCoords()\n",
    "        c = self.flag.getCoords()\n",
    "        dxi, dyi = b[0] - a[0], b[1] - a[1]\n",
    "        dxf, dyf = c[0] - a[0], c[1] - a[1]\n",
    "        f_angle = math.atan2(dyf, dxf)\n",
    "        i_angle = math.atan2(dyi, dxi)\n",
    "        return [f_angle - self.ship.direction, self.target.get_dist(a[0], a[1]), i_angle - self.ship.direction, self.flag.get_dist(a[0], a[1])]\n",
    "\n",
    "    def build_environment(self):\n",
    "        self.n_angles = 0\n",
    "        rate = random.uniform(0.4, 0.6)\n",
    "        dist = random.uniform(7, 15)\n",
    "        angle = random.uniform(0, 2*math.pi)\n",
    "        self.flag = Island(0, 0, 1)\n",
    "        self.ship = Ship(-10, 0)\n",
    "        self.target = Target(-5, 5, 1, -math.pi / 2, 1)\n",
    "        self.ship.direction = 0\n",
    "        self.prev_dist = self.flag.get_dist(self.ship.x, self.ship.y)\n",
    "        self.prev_i_dist = self.target.get_dist(self.ship.x, self.ship.y)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.build_environment()\n",
    "        self.d = {}\n",
    "        self.i = 0\n",
    "        return self.get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        dt = 1\n",
    "        \n",
    "        state = self.ship.getCoords()\n",
    "\n",
    "        if action == 0:\n",
    "            self.ship.direction += math.pi / 6\n",
    "            self.n_angles += 1\n",
    "        elif action == 1:\n",
    "            self.ship.direction -= math.pi / 6\n",
    "            self.n_angles += 1\n",
    "        # elif action == 2:\n",
    "        #     self.ship.v += 0.25\n",
    "        # elif action == 3:\n",
    "        #     self.ship.v -= 0.25\n",
    "        \n",
    "        self.ship.move(dt)\n",
    "        self.target.move(dt)\n",
    "        \n",
    "        self.ship.add_position()\n",
    "\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        self.i += 1\n",
    "        \n",
    "        if self.flag.belongs_to_boarder(self.ship.x, self.ship.y):\n",
    "            reward = 20000\n",
    "            done = True\n",
    "            \n",
    "            self.d = {}\n",
    "            self.i = 0\n",
    "            reward -= self.n_angles * 500\n",
    "\n",
    "        elif self.target.belongs_to_boarder(self.ship.x, self.ship.y):\n",
    "            reward = -10000\n",
    "            done = True\n",
    "\n",
    "            self.d = {}\n",
    "            self.i = 0\n",
    "            reward -= self.n_angles * 500\n",
    "            \n",
    "        elif len(self.ship.get_positions()) > 25:\n",
    "            done = True\n",
    "            self.d = {}\n",
    "            self.i = 0\n",
    "            # print(\"Time limit exceeded\")\n",
    "            reward = -1000 * (self.prev_dist - self.flag.get_dist(self.ship.x, self.ship.y)) ** 2\n",
    "            reward -= self.n_angles * 500\n",
    "\n",
    "        else:\n",
    "            reward = (self.prev_dist - self.flag.get_dist(self.ship.x, self.ship.y)) ** 3\n",
    "            reward -= 0.1 * (self.prev_i_dist - self.target.get_dist(self.ship.x, self.ship.y)) ** 3\n",
    "            if reward < 0:\n",
    "                reward *= 10\n",
    "            # reward += 10 * (1 - self.ship.cum_d ** 2) * math.exp( - 0.5 * self.ship.cum_d ** 2) - 5\n",
    "            done = False\n",
    "        \n",
    "        self.prev_dist = self.flag.get_dist(self.ship.x, self.ship.y)\n",
    "        self.prev_i_dist = self.target.get_dist(self.ship.x, self.ship.y)\n",
    "        # self.draw_map()\n",
    "        return next_state, reward, done \n",
    "    \n",
    "    def draw_map(self):\n",
    "        field, ax = plt.subplots()\n",
    "        ax.set(xlim=(x_upper_bound * -2, x_upper_bound * 2), ylim=(y_upper_bound * -2, y_upper_bound * 2))\n",
    "        ax.set_aspect(1)\n",
    "        ax.add_artist(self.target.draw('green'))\n",
    "        ax.add_artist(self.flag.draw_island())\n",
    "        pos = self.ship.get_positions()\n",
    "        for i in range(len(pos) - 1):\n",
    "            ax.plot([pos[i][0], pos[i + 1][0]], [pos[i][1], pos[i + 1][1]], marker = 'o', color='r')\n",
    "        plt.show()\n",
    "\n",
    "    def final(self):\n",
    "        print('The shortest route:', self.shortest)\n",
    "        print('The longest route:', self.longest)\n",
    "\n",
    "def final_states():\n",
    "    return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a8ad7-ee4e-42d5-98d7-664f0ebbc029",
   "metadata": {},
   "source": [
    "Ключевая идея: подаем на вектор относительных координат острова и флага, а также пеленги на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8127c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LR = 0.03\n",
    "GAMMA = 0.90\n",
    "EPSILON = 0.9\n",
    "MEMORY_CAPACITY = 10000\n",
    "Q_NETWORK_ITERATION = 1000\n",
    "\n",
    "env = Environment()\n",
    "NUM_ACTIONS = env.n_actions\n",
    "NUM_STATES = 4\n",
    "EPISODES = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a6105-17c7-486a-b2de-6740ffdc0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_ACTIONS,\n",
    "                 input_size=NUM_STATES,\n",
    "                 hidden_size=NUM_STATES-2,\n",
    "                 num_layers=2):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        # c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78146c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUM_STATES, 80)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2 = nn.Linear(80, 160)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "        self.fc3 = nn.Linear(160, 80)\n",
    "        self.fc3.weight.data.normal_(0, 0.1)\n",
    "        self.fc4 = nn.Linear(80, NUM_ACTIONS)\n",
    "        self.fc4.weight.data.normal_(0, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727b66e-72c0-486e-aa44-873aeea9701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUM_STATES, 80)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2 = nn.Linear(80, NUM_ACTIONS)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES * 2 + 2))\n",
    "        self.memory_counter = 0\n",
    "        self.learn_counter = 0\n",
    "        self.optimizer = optim.Adam(self.eval_net.parameters(), LR)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def store_trans(self, state, action, reward, next_state):\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        trans = np.hstack((state, action, reward, next_state))\n",
    "        self.memory[index, :] = trans\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0)\n",
    "        if np.random.randn() <= EPSILON:\n",
    "            action_value = self.eval_net.forward(state)\n",
    "            action = torch.max(action_value, 1)[1].data.numpy()\n",
    "            action = action[0]\n",
    "        else:\n",
    "            action = np.random.randint(0, NUM_ACTIONS)\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        # learn 100 times then the target network update\n",
    "        if self.learn_counter % Q_NETWORK_ITERATION == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_counter+=1\n",
    "\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n",
    "        #note that the action must be a int\n",
    "        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n",
    "        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES+1: NUM_STATES+2])\n",
    "        batch_next_state = torch.FloatTensor(batch_memory[:, -NUM_STATES:])\n",
    "\n",
    "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
    "        q_next = self.target_net(batch_next_state).detach()\n",
    "        q_target = batch_reward + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "\n",
    "        loss = self.loss(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34c40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "net = DQN()\n",
    "\n",
    "def main():\n",
    "    print(\"The DQN is collecting experience...\")\n",
    "    step_counter_list = []\n",
    "    for episode in tqdm.tqdm(range(EPISODES)):\n",
    "        state = env.reset()\n",
    "        step_counter = 0\n",
    "        sum_rew = 0\n",
    "        while True:\n",
    "            step_counter +=1\n",
    "            action = net.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            # reward = reward * 100 if reward > 0 else reward * 5\n",
    "            sum_rew += reward\n",
    "            net.store_trans(state, action, reward, next_state)\n",
    "            if net.memory_counter >= MEMORY_CAPACITY:\n",
    "                net.learn()\n",
    "            if done:\n",
    "                step_counter_list.append(step_counter)\n",
    "                # print(\"episode {}, the reward is {}\".format(episode, round(reward, 3)))\n",
    "                rewards.append(sum_rew)\n",
    "                if episode % 10 == 0:\n",
    "                    env.draw_map()\n",
    "                # net.plot(net.ax, step_counter_list)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84cd67-0ab1-4812-92ac-ddcd2a663d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29febe8f-ae85-4160-933a-53669cc08c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "p = 0\n",
    "for i in rewards:\n",
    "    if i < 0:\n",
    "        m += 1\n",
    "    else:\n",
    "        p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2e75b-761a-4eed-9ec4-a5994c81601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m / (m + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13890e2d-c4e7-4ec6-adf6-a83eb0a5bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.draw_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions\n",
    "        print(self.actions)\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        self.q_table_final = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        self.check_state_exist(next_state)\n",
    "\n",
    "        q_predict = self.q_table.loc[state, action]\n",
    "\n",
    "        if next_state != 'goal' or next_state != 'obstacle':\n",
    "            q_target = reward + self.gamma * self.q_table.loc[next_state, :].max()\n",
    "        else:\n",
    "            q_target = reward\n",
    "        self.q_table.loc[state, action] += self.lr * (q_target - q_predict)\n",
    "\n",
    "        return self.q_table.loc[state, action]\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "                )\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_KT",
   "language": "python",
   "name": "rl_kt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
