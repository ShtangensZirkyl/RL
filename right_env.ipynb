{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410c7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270244e1-6875-4b46-9476-2d87eabf9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Island:\n",
    "    def __init__(self, xcentr=5, ycentr=5, radius=1):\n",
    "        self.xcentr = xcentr\n",
    "        self.ycentr = ycentr\n",
    "        self.radius = radius\n",
    "\n",
    "    def belongs_to_boarder(self, x, y):\n",
    "        dist = ((x - self.xcentr) ** 2 + (y - self.ycentr) ** 2) ** 0.5\n",
    "        if dist <= self.radius:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_dist(self, x, y):\n",
    "        return ((x - self.xcentr) ** 2 + (y - self.ycentr) ** 2) ** 0.5 - self.radius\n",
    "\n",
    "    def draw_island(self, color='blue'):\n",
    "        return plt.Circle((self.xcentr, self.ycentr), self.radius, color=color)\n",
    "\n",
    "    def getCoords(self):\n",
    "        return [self.xcentr, self.ycentr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31785e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lower_bound = 0\n",
    "x_upper_bound = 10\n",
    "y_lower_bound = 0\n",
    "y_upper_bound = 10\n",
    "\n",
    "a = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32cc8f2-273e-47b3-b737-a416e9072088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a040072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ship:\n",
    "    def __init__(self, x = 1, y = 5, v = 1, direction = 0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.v = v\n",
    "        self.direction = direction\n",
    "        self.prev_d = self.direction\n",
    "        self.positions = [[self.x, self.y]]\n",
    "        self.cum_d = 0\n",
    "        \n",
    "    def move(self, dt=1):\n",
    "        self.x += dt * self.v * math.cos(self.direction)\n",
    "        self.y += dt * self.v * math.sin(self.direction)\n",
    "        self.cum_d += abs(self.direction - self.prev_d)\n",
    "        self.prev_d = self.direction\n",
    "        \n",
    "    def getCoords(self):\n",
    "        return [self.x, self.y]\n",
    "    \n",
    "    def add_position(self):\n",
    "        self.positions.append([self.x, self.y])\n",
    "        \n",
    "    def get_positions(self):\n",
    "        return self.positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0521dd18-a193-48d9-9376-dd7e8ff44063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__()\n",
    "        self.action_space = ['left', 'right', 'idle']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.n_features = 2\n",
    "\n",
    "        self.build_environment()\n",
    "        \n",
    "        self.route = []\n",
    "        self.list_of_route = []\n",
    "        self.d = {}\n",
    "        self.f = {}\n",
    "        \n",
    "        self.prev_dist = 0\n",
    "        self.prev_i_dist = 0\n",
    "\n",
    "        self.i = 0\n",
    "\n",
    "        self.c = True\n",
    "\n",
    "        self.longest = 0\n",
    "\n",
    "        self.shortest = 0\n",
    "        \n",
    "    def get_state(self):\n",
    "        a = self.ship.getCoords()\n",
    "        b = self.island.getCoords()\n",
    "        c = self.flag.getCoords()\n",
    "        dxi, dyi = b[0] - a[0], b[1] - a[1]\n",
    "        dxf, dyf = c[0] - a[0], c[1] - a[1]\n",
    "        f_angle = math.atan2(dyf, dxf)\n",
    "        i_angle = math.atan2(dyi, dxi)\n",
    "        return [f_angle - self.ship.direction, f_angle, dxf, dyf, i_angle - self.ship.direction, i_angle, dxi, dyi]\n",
    "\n",
    "    def build_environment(self):\n",
    "        rate = random.uniform(0.3, 0.5)\n",
    "        dist = random.uniform(7, 15)\n",
    "        angle = math.pi\n",
    "        self.flag = Island(0, 0, 1)\n",
    "        self.ship = Ship(dist * math.cos(angle), dist * math.sin(angle))\n",
    "        self.island = Island(rate * dist * math.cos(angle), rate * dist * math.sin(angle))\n",
    "        a = self.ship.getCoords()\n",
    "        c = self.flag.getCoords()\n",
    "        dxf, dyf = c[0] - a[0], c[1] - a[1]\n",
    "        f_angle = math.atan2(dyf, dxf)\n",
    "        self.ship.direction = f_angle\n",
    "        self.prev_dist = self.flag.get_dist(self.ship.x, self.ship.y)\n",
    "        self.prev_i_dist = self.island.get_dist(self.ship.x, self.ship.y)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.build_environment()\n",
    "        self.d = {}\n",
    "        self.i = 0\n",
    "        return self.get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        dt = 1\n",
    "        \n",
    "        state = self.ship.getCoords()\n",
    "\n",
    "        if action == 0:\n",
    "            self.ship.direction += math.pi / 6\n",
    "        elif action == 1:\n",
    "            self.ship.direction -= math.pi / 6\n",
    "        # elif action == 2:\n",
    "        #     self.ship.v += 0.25\n",
    "        # elif action == 3:\n",
    "        #     self.ship.v -= 0.25\n",
    "        \n",
    "        self.ship.move(dt)\n",
    "        \n",
    "        self.ship.add_position()\n",
    "\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        self.i += 1\n",
    "        \n",
    "        if self.flag.belongs_to_boarder(self.ship.x, self.ship.y):\n",
    "            reward = 20000\n",
    "            done = True\n",
    "            \n",
    "            self.d = {}\n",
    "            self.i = 0\n",
    "\n",
    "        elif self.island.belongs_to_boarder(self.ship.x, self.ship.y):\n",
    "            reward = -10000\n",
    "            done = True\n",
    "\n",
    "            self.d = {}\n",
    "            self.i = 0\n",
    "            \n",
    "        elif len(self.ship.get_positions()) > 500:\n",
    "            done = True\n",
    "            self.d = {}\n",
    "            self.i = 0\n",
    "            # print(\"Time limit exceeded\")\n",
    "            reward = -1000 * (self.prev_dist - self.flag.get_dist(self.ship.x, self.ship.y)) ** 2\n",
    "\n",
    "        else:\n",
    "            reward = (self.prev_dist - self.flag.get_dist(self.ship.x, self.ship.y)) ** 3\n",
    "            reward -= 0.1 * (self.prev_i_dist - self.island.get_dist(self.ship.x, self.ship.y)) ** 3\n",
    "            if reward < 0:\n",
    "                reward *= 10\n",
    "            # reward += 10 * (1 - self.ship.cum_d ** 2) * math.exp( - 0.5 * self.ship.cum_d ** 2) - 5\n",
    "            done = False\n",
    "        \n",
    "        self.prev_dist = self.flag.get_dist(self.ship.x, self.ship.y)\n",
    "        self.prev_i_dist = self.island.get_dist(self.ship.x, self.ship.y)\n",
    "\n",
    "        return next_state, reward, done \n",
    "    \n",
    "    def draw_map(self):\n",
    "        field, ax = plt.subplots()\n",
    "        ax.set(xlim=(x_upper_bound * -2, x_upper_bound * 2), ylim=(y_upper_bound * -2, y_upper_bound * 2))\n",
    "        ax.set_aspect(1)\n",
    "        ax.add_artist(self.island.draw_island('green'))\n",
    "        ax.add_artist(self.flag.draw_island())\n",
    "        pos = self.ship.get_positions()\n",
    "        for i in range(len(pos) - 1):\n",
    "            ax.plot([pos[i][0], pos[i + 1][0]], [pos[i][1], pos[i + 1][1]], marker = 'o', color='r')\n",
    "        plt.show()\n",
    "\n",
    "    def final(self):\n",
    "        print('The shortest route:', self.shortest)\n",
    "        print('The longest route:', self.longest)\n",
    "\n",
    "def final_states():\n",
    "    return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a8ad7-ee4e-42d5-98d7-664f0ebbc029",
   "metadata": {},
   "source": [
    "Ключевая идея: подаем на вектор относительных координат острова и флага, а также пеленги на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b8127c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LR = 0.03\n",
    "GAMMA = 0.90\n",
    "EPSILON = 0.9\n",
    "MEMORY_CAPACITY = 10000\n",
    "Q_NETWORK_ITERATION = 1000\n",
    "\n",
    "env = Environment()\n",
    "NUM_ACTIONS = env.n_actions\n",
    "NUM_STATES = 8\n",
    "EPISODES = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "764a6105-17c7-486a-b2de-6740ffdc0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_ACTIONS,\n",
    "                 input_size=NUM_STATES,\n",
    "                 hidden_size=NUM_STATES-2,\n",
    "                 num_layers=2):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        # c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78146c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUM_STATES, 80)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2 = nn.Linear(80, 160)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "        self.fc3 = nn.Linear(160, 80)\n",
    "        self.fc3.weight.data.normal_(0, 0.1)\n",
    "        self.fc4 = nn.Linear(80, NUM_ACTIONS)\n",
    "        self.fc4.weight.data.normal_(0, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d727b66e-72c0-486e-aa44-873aeea9701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(NUM_STATES, 80)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2 = nn.Linear(80, NUM_ACTIONS)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c29f03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = LSTM1(), LSTM1()\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, NUM_STATES * 2 + 2))\n",
    "        self.memory_counter = 0\n",
    "        self.learn_counter = 0\n",
    "        self.optimizer = optim.Adam(self.eval_net.parameters(), LR)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def store_trans(self, state, action, reward, next_state):\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        trans = np.hstack((state, action, reward, next_state))\n",
    "        self.memory[index, :] = trans\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state), 0)\n",
    "        if np.random.randn() <= EPSILON:\n",
    "            action_value = self.eval_net.forward(state)\n",
    "            action = torch.max(action_value, 1)[1].data.numpy()\n",
    "            action = action[0]\n",
    "        else:\n",
    "            action = np.random.randint(0, NUM_ACTIONS)\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        # learn 100 times then the target network update\n",
    "        if self.learn_counter % Q_NETWORK_ITERATION == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_counter+=1\n",
    "\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        batch_state = torch.FloatTensor(batch_memory[:, :NUM_STATES])\n",
    "        #note that the action must be a int\n",
    "        batch_action = torch.LongTensor(batch_memory[:, NUM_STATES:NUM_STATES+1].astype(int))\n",
    "        batch_reward = torch.FloatTensor(batch_memory[:, NUM_STATES+1: NUM_STATES+2])\n",
    "        batch_next_state = torch.FloatTensor(batch_memory[:, -NUM_STATES:])\n",
    "\n",
    "        q_eval = self.eval_net(batch_state).gather(1, batch_action)\n",
    "        q_next = self.target_net(batch_next_state).detach()\n",
    "        q_target = batch_reward + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "\n",
    "        loss = self.loss(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b34c40d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DQN is collecting experience...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16436/1034674467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16436/1034674467.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mstep_counter\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# reward = reward * 100 if reward > 0 else reward * 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16436/2934338282.py\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0maction_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16436/2134216236.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Propagate input through LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#lstm with input, hidden, and internal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mhn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reshaping the data for Dense layer next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    630\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                            ):\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0;32m    634\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\RL\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mexpected_input_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_input_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    202\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m    203\u001b[0m                     expected_input_dim, input.dim()))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "net = DQN()\n",
    "\n",
    "def main():\n",
    "    print(\"The DQN is collecting experience...\")\n",
    "    step_counter_list = []\n",
    "    for episode in tqdm.tqdm(range(EPISODES)):\n",
    "        state = env.reset()\n",
    "        step_counter = 0\n",
    "        sum_rew = 0\n",
    "        while True:\n",
    "            step_counter +=1\n",
    "            action = net.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            # reward = reward * 100 if reward > 0 else reward * 5\n",
    "            sum_rew += reward\n",
    "            net.store_trans(state, action, reward, next_state)\n",
    "            if net.memory_counter >= MEMORY_CAPACITY:\n",
    "                net.learn()\n",
    "            if done:\n",
    "                step_counter_list.append(step_counter)\n",
    "                # print(\"episode {}, the reward is {}\".format(episode, round(reward, 3)))\n",
    "                rewards.append(sum_rew)\n",
    "                if episode % 100 == 0:\n",
    "                    env.draw_map()\n",
    "                # net.plot(net.ax, step_counter_list)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84cd67-0ab1-4812-92ac-ddcd2a663d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29febe8f-ae85-4160-933a-53669cc08c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "p = 0\n",
    "for i in rewards:\n",
    "    if i < 0:\n",
    "        m += 1\n",
    "    else:\n",
    "        p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30e2e75b-761a-4eed-9ec4-a5994c81601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13715"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m / (m + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9773f13e-32fd-40a7-9da2-714858a1ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17257"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions\n",
    "        print(self.actions)\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        self.q_table_final = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        self.check_state_exist(next_state)\n",
    "\n",
    "        q_predict = self.q_table.loc[state, action]\n",
    "\n",
    "        if next_state != 'goal' or next_state != 'obstacle':\n",
    "            q_target = reward + self.gamma * self.q_table.loc[next_state, :].max()\n",
    "        else:\n",
    "            q_target = reward\n",
    "        self.q_table.loc[state, action] += self.lr * (q_target - q_predict)\n",
    "\n",
    "        return self.q_table.loc[state, action]\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "                )\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_KT",
   "language": "python",
   "name": "rl_kt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
