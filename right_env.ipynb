{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87be4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89158208",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lower_bound = 0\n",
    "x_upper_bound = 10\n",
    "y_lower_bound = 0\n",
    "y_upper_bound = 10\n",
    "\n",
    "a = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17160f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Island:\n",
    "    def __init__(self, xcentr = 5, ycentr = 5, radius = 1):\n",
    "        self.xcentr = xcentr\n",
    "        self.ycentr = ycentr\n",
    "        self.radius = radius\n",
    "    def belongs_to_boarder(self, x, y):\n",
    "        if ((x - self.xcentr)**2 + (y - self.ycentr)**2 <= radius**2):\n",
    "            return true\n",
    "        else:\n",
    "            return false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b54e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ship:\n",
    "    def __init__(self, x = 1, y = 5, vx = 0, vy = 0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vx = vx\n",
    "        self.vy = vy\n",
    "    def move(self):\n",
    "        self.x = self.x + self.vx * 1\n",
    "        self.y = self.y + self.vy * 1\n",
    "    def getCoords(self):\n",
    "        return [self.x, self.y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1715e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating class for the environment\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        super(Environment, self).__init__()\n",
    "        self.action_space = ['up', 'down', 'left', 'right']\n",
    "        self.n_actions = len(self.action_space)\n",
    "\n",
    "        self.build_environment()\n",
    "\n",
    "        # Dictionaries to draw the final route\n",
    "        self.d = {}\n",
    "        self.f = {}\n",
    "\n",
    "        # Key for the dictionaries\n",
    "        self.i = 0\n",
    "\n",
    "        # Writing the final dictionary first time\n",
    "        self.c = True\n",
    "\n",
    "        # Showing the steps for longest found route\n",
    "        self.longest = 0\n",
    "\n",
    "        # Showing the steps for the shortest route\n",
    "        self.shortest = 0\n",
    "\n",
    "    # Function to build the environment\n",
    "    def build_environment(self):\n",
    "        self.island = Island()\n",
    "        \n",
    "        self.field, self.ax = plt.subplots()\n",
    "        self.ax.set(xlim=(x_lower_bound, x_upper_bound), ylim=(y_lower_bound, y_upper_bound))\n",
    "        \n",
    "        self.ax.set_aspect(1)\n",
    "        \n",
    "        self.obstacle_image = plt.Circle((self.island.xcentr, self.island.ycentr), self.island.radius)\n",
    "        self.ax.add_artist(self.obstacle_image)\n",
    "\n",
    "        # Final Point \n",
    "        self.flag = Island(9.5, 5, 0.5)\n",
    "        self.flag_image = plt.Circle((self.flag.xcentr, self.flag.ycentr), self.flag.radius)\n",
    "        self.ax.add_artist(self.flag_image)\n",
    "        \n",
    "        self.ship = Ship()\n",
    "        self.ship_image = plt.Circle((self.ship.x, self.ship.y), 0.1)\n",
    "        self.ax.add_artist(self.ship_image)\n",
    "\n",
    "        plt.show()\n",
    "    \"\"\"\n",
    "    # Function to reset the environment and start new Episode\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "\n",
    "        # Updating agent\n",
    "        self.canvas_widget.delete(self.agent)\n",
    "        self.ship = self.canvas_widget.create_image(0, pixels * 4, anchor='nw', image=self.robot)\n",
    "\n",
    "        # # Clearing the dictionary and the i\n",
    "        self.d = {}\n",
    "        self.i = 0\n",
    "\n",
    "        # Return observation\n",
    "        return self.canvas_widget.coords(self.agent)\n",
    "    \"\"\"\n",
    "    # Function to get the next observation and reward by doing next step\n",
    "    def step(self, action):\n",
    "        # Current state of the agent\n",
    "        state = self.ship.getCoords()\n",
    "        base_action = np.array([0, 0])\n",
    "\n",
    "        # Updating next state according to the action\n",
    "        # Action 'up'\n",
    "        if action == 0:\n",
    "            if state[1] <= y_upper_bound:\n",
    "                base_action[1] += 1\n",
    "        # Action 'down'\n",
    "        elif action == 1:\n",
    "            if state[1] > y_lower_bound:\n",
    "                base_action[1] -= 1\n",
    "        # Action right\n",
    "        elif action == 2:\n",
    "            if state[0] < x_upper_bound:\n",
    "                base_action[0] += 1\n",
    "        # Action left\n",
    "        elif action == 3:\n",
    "            if state[0] >= x_lower_bound:\n",
    "                base_action[0] -= 1\n",
    "\n",
    "        # Moving the agent according to the action\n",
    "        self.ship_image = plt.Circle((self.ship.x + base_action[0], self.ship.y + base_action[1]), 0.1)\n",
    "        self.ax.add_artist(ship_image)\n",
    "\n",
    "        # Writing in the dictionary coordinates of found route\n",
    "        self.d[self.i] = self.ship.getCoords()\n",
    "\n",
    "        # Updating next state\n",
    "        next_state = self.d[self.i]\n",
    "\n",
    "        # Updating key for the dictionary\n",
    "        self.i += 1\n",
    "\n",
    "        # Calculating the reward for the agent\n",
    "        if (self.flag.belongs_to_boarder(next_state[0], next_state[1])):\n",
    "            reward = 1\n",
    "            done = True\n",
    "            next_state = 'goal'\n",
    "\n",
    "            # Filling the dictionary first time\n",
    "            if self.c == True:\n",
    "                for j in range(len(self.d)):\n",
    "                    self.f[j] = self.d[j]\n",
    "                self.c = False\n",
    "                self.longest = len(self.d)\n",
    "                self.shortest = len(self.d)\n",
    "\n",
    "            # Checking if the currently found route is shorter\n",
    "            if len(self.d) < len(self.f):\n",
    "                # Saving the number of steps for the shortest route\n",
    "                self.shortest = len(self.d)\n",
    "                # Clearing the dictionary for the final route\n",
    "                self.f = {}\n",
    "                # Reassigning the dictionary\n",
    "                for j in range(len(self.d)):\n",
    "                    self.f[j] = self.d[j]\n",
    "\n",
    "            # Saving the number of steps for the longest route\n",
    "            if len(self.d) > self.longest:\n",
    "                self.longest = len(self.d)\n",
    "\n",
    "        elif (self.island.belongs_to_boarder(next_state[0], next_state[1])):\n",
    "            reward = -1\n",
    "            done = True\n",
    "            next_state = 'obstacle'\n",
    "\n",
    "            # Clearing the dictionary and the i\n",
    "            self.d = {}\n",
    "            self.i = 0\n",
    "\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "        return next_state, reward, done\n",
    "    \"\"\"\n",
    "    # Function to refresh the environment\n",
    "    def render(self):\n",
    "        #time.sleep(0.03)\n",
    "        self.update()\n",
    "\n",
    "    # Function to show the found route\n",
    "    def final(self):\n",
    "        # Deleting the agent at the end\n",
    "        self.canvas_widget.delete(self.agent)\n",
    "\n",
    "        # Showing the number of steps\n",
    "        print('The shortest route:', self.shortest)\n",
    "        print('The longest route:', self.longest)\n",
    "\n",
    "        # Creating initial point\n",
    "        origin = np.array([20, 180])\n",
    "        self.initial_point = self.canvas_widget.create_oval(\n",
    "            origin[0] - 5, origin[1] - 5,\n",
    "            origin[0] + 5, origin[1] + 5,\n",
    "            fill='blue', outline='blue')\n",
    "\n",
    "        # Filling the route\n",
    "        for j in range(len(self.f)):\n",
    "            # Showing the coordinates of the final route\n",
    "            print(self.f[j])\n",
    "            self.track = self.canvas_widget.create_oval(\n",
    "                self.f[j][0] + origin[0] - 5, self.f[j][1] + origin[0] - 5,\n",
    "                self.f[j][0] + origin[0] + 5, self.f[j][1] + origin[0] + 5,\n",
    "                fill='blue', outline='blue')\n",
    "            # Writing the final route in the global variable a\n",
    "            a[j] = self.f[j]\n",
    "\n",
    "\n",
    "# Returning the final dictionary with route coordinates\n",
    "# Then it will be used in agent_brain.py\n",
    "def final_states():\n",
    "    return a\n",
    "\n",
    "\"\"\"\n",
    "# This we need to debug the environment\n",
    "# If we want to run and see the environment without running full algorithm\n",
    "if __name__ == '__main__':\n",
    "    env = Environment()\n",
    "    # env.build_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        # List of actions\n",
    "        self.actions = actions\n",
    "        # Learning rate\n",
    "        self.lr = learning_rate\n",
    "        # Value of gamma\n",
    "        self.gamma = reward_decay\n",
    "        # Value of epsilon\n",
    "        self.epsilon = e_greedy\n",
    "        # Creating full Q-table for all cells\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        # Creating Q-table for cells of the final route\n",
    "        self.q_table_final = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    # Function for choosing the action for the agent\n",
    "    def choose_action(self, observation):\n",
    "        # Checking if the state exists in the table\n",
    "        self.check_state_exist(observation)\n",
    "        # Selection of the action - 90 % according to the epsilon == 0.9\n",
    "        # Choosing the best action\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # Choosing random action - left 10 % for choosing randomly\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    # Function for learning and updating Q-table with new knowledge\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Checking if the next step exists in the Q-table\n",
    "        self.check_state_exist(next_state)\n",
    "\n",
    "        # Current state in the current position\n",
    "        q_predict = self.q_table.loc[state, action]\n",
    "\n",
    "        # Checking if the next state is free or it is obstacle or goal\n",
    "        if next_state != 'goal' or next_state != 'obstacle':\n",
    "            q_target = reward + self.gamma * self.q_table.loc[next_state, :].max()\n",
    "        else:\n",
    "            q_target = reward\n",
    "\n",
    "        # Updating Q-table with new knowledge\n",
    "        self.q_table.loc[state, action] += self.lr * (q_target - q_predict)\n",
    "\n",
    "        return self.q_table.loc[state, action]\n",
    "\n",
    "    # Adding to the Q-table new states\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Printing the Q-table with states\n",
    "    def print_q_table(self):\n",
    "        # Getting the coordinates of final route from env.py\n",
    "        e = final_states()\n",
    "\n",
    "        # Comparing the indexes with coordinates and writing in the new Q-table values\n",
    "        for i in range(len(e)):\n",
    "            state = str(e[i])  # state = '[5.0, 40.0]'\n",
    "            # Going through all indexes and checking\n",
    "            for j in range(len(self.q_table.index)):\n",
    "                if self.q_table.index[j] == state:\n",
    "                    self.q_table_final.loc[state, :] = self.q_table.loc[state, :]\n",
    "\n",
    "        print()\n",
    "        print('Length of final Q-table =', len(self.q_table_final.index))\n",
    "        print('Final Q-table with values from the final route:')\n",
    "        print(self.q_table_final)\n",
    "\n",
    "        print()\n",
    "        print('Length of full Q-table =', len(self.q_table.index))\n",
    "        print('Full Q-table:')\n",
    "        print(self.q_table)\n",
    "\n",
    "    # Plotting the results for the number of steps\n",
    "    def plot_results(self, steps, cost):\n",
    "        #\n",
    "        f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "        #\n",
    "        ax1.plot(np.arange(len(steps)), steps, 'b')\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Steps')\n",
    "        ax1.set_title('Episode via steps')\n",
    "\n",
    "        #\n",
    "        ax2.plot(np.arange(len(cost)), cost, 'r')\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Cost')\n",
    "        ax2.set_title('Episode via cost')\n",
    "\n",
    "        plt.tight_layout()  # Function to make distance between figures\n",
    "\n",
    "        #\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(steps)), steps, 'b')\n",
    "        plt.title('Episode via steps')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "\n",
    "        #\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(cost)), cost, 'r')\n",
    "        plt.title('Episode via cost')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cost')\n",
    "\n",
    "        # Showing the plots\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
